{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTA3qNexKv3"
      },
      "source": [
        "## Supplementary Material\n",
        "Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications\n",
        "\n",
        "\n",
        "### EEGTransformer Class\n",
        "\n",
        "The `EEGTransformer` class is designed to leverage a transformer-based architecture tailored specifically for Electroencephalogram (EEG) data processing.\n",
        "\n",
        "#### Parameters:\n",
        "- `num_channels` (int): Specifies the number of channels in the EEG dataset.\n",
        "- `num_timepoints` (int): Indicates the number of time points or the sequence length in the EEG data.\n",
        "- `output_dim` (int): Defines the output dimensionality for the classifier layer.\n",
        "- `hidden_dim` (int): Specifies the hidden layer dimensionality.\n",
        "- `num_heads` (int): Determines the number of attention heads to be used in the multi-head self-attention mechanism.\n",
        "- `key_query_dim` (int): Denotes the dimensionality for the key/query pairs in the self-attention mechanism.\n",
        "- `hidden_ffn_dim` (int): Indicates the hidden layer dimensionality for the feed-forward network.\n",
        "- `intermediate_dim` (int): Refers to the dimensionality of the intermediate layer in the feed-forward network.\n",
        "- `ffn_output_dim` (int): Specifies the output size of the feed-forward network.\n",
        "\n",
        "#### Attributes:\n",
        "- `positional_encoding` (torch.Tensor): A tensor of shape `(num_channels, num_timepoints)` that imparts the sequence position information.\n",
        "- `multihead_attn` (nn.MultiheadAttention): Implements the multi-head self-attention mechanism.\n",
        "- `ffn` (nn.Sequential): Constructs a feed-forward network composed of a linear transformation followed by ReLU activation and another linear transformation.\n",
        "- `norm1` and `norm2` (nn.LayerNorm): Execute layer normalization.\n",
        "- `classifier` (nn.Linear): Deploys a final linear transformation layer to categorize the input into designated classes.\n",
        "\n",
        "#### Methods:\n",
        "- `forward(X)`: Outlines the forward propagation for the model.\n",
        "  - `X` (torch.Tensor): The input tensor for EEG data, which should have a shape of `(batch_size, num_channels, num_timepoints)`.\n",
        "\n",
        "  - **Steps**:\n",
        "    1. Standardize the input tensor.\n",
        "    2. Apply positional encoding.\n",
        "    3. Implement multi-head self-attention.\n",
        "    4. Reshape the attention output and apply layer normalization.\n",
        "    5. Forward the data through the feed-forward network.\n",
        "    6. Flatten the resultant tensor and direct it through a classifier layer.\n",
        "    7. Yield the final output.\n",
        "  \n",
        "### Notes:\n",
        "\n",
        "- The model applies layer normalization after the multi-head self-attention and feed-forward network stages.\n",
        "- Positional encoding is utilized to impart sequence position information to the model, which can either be relative or absolute.\n",
        "- The classifier layer flattens the model output and categorizes it into `output_dim` classes.\n",
        "\n",
        "### Usage:\n",
        "\n",
        "To employ the `EEGTransformer` model, instantiate the class using the desired parameters. Then, similar to any other PyTorch model, forward the input data to the model and utilize the returned output for either training or inference.\n",
        "\n",
        "```python\n",
        "# Sample Usage\n",
        "model = EEGTransformer(num_channels=32, num_timepoints=200, output_dim=2,\n",
        "                       hidden_dim=512, num_heads=8, key_query_dim=512,\n",
        "                       hidden_ffn_dim=512, intermediate_dim=2048,\n",
        "                       ffn_output_dim=32)\n",
        "                       \n",
        "input_data = torch.randn(64, 32, 200)\n",
        "output = model(input_data)\n",
        "```\n",
        "\n",
        "Ensure that the model is paired with a compatible loss function and optimizer for effective training. Depending on the specifics of the EEG dataset or application requirements, the model can be further refined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Supplementary Material\n",
        "# Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications\n",
        "#\n",
        "# ### EEGTransformer Class\n",
        "#\n",
        "# This notebook contains a TensorFlow/Keras re-implementation of the `EEGTransformer` class, originally presented in PyTorch. It is designed to be a faithful translation, allowing for integration into a TensorFlow-based pipeline.\n",
        "\n",
        "# Cell 2: Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# ### TensorFlow/Keras EEGTransformer Implementation\n",
        "#\n",
        "# This cell contains the complete and corrected class definitions. \n",
        "# The PositionalEncoding layer now creates the encoding matrix directly in the call() method.\n",
        "# This is the most robust solution to prevent TensorFlow's InaccessibleTensorError by ensuring\n",
        "# the tensor is always created in the correct graph scope.\n",
        "\n",
        "# Cell 4: PositionalEncoding and EEGTransformer Class Definitions\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Keras layer to create and apply positional encodings.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        This method is called for every forward pass.\n",
        "        \"\"\"\n",
        "        # --- Create the positional encoding matrix on the fly ---\n",
        "        # The input shape is (batch, timepoints, channels)\n",
        "        _, num_timepoints, num_channels = inputs.shape\n",
        "        \n",
        "        positions = tf.range(start=0, limit=num_timepoints, delta=1, dtype=tf.float32)\n",
        "        channels = tf.range(start=0, limit=num_channels, delta=1, dtype=tf.float32)\n",
        "        \n",
        "        angle_rates = 1 / (10000 ** ((2 * (channels // 2)) / tf.cast(num_channels, tf.float32)))\n",
        "        angle_rads = positions[:, tf.newaxis] * angle_rates[tf.newaxis, :]\n",
        "        \n",
        "        sines = tf.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.cos(angle_rads[:, 1::2])\n",
        "        \n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        \n",
        "        # Add the encoding to the input tensor\n",
        "        return inputs + tf.cast(pos_encoding, dtype=inputs.dtype)\n",
        "\n",
        "class EEGTransformer(keras.Model):\n",
        "    \"\"\"\n",
        "    TensorFlow/Keras model for EEG data using a Transformer architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_channels, output_dim,\n",
        "                 num_heads, key_dim, ffn_intermediate_dim, dropout_rate=0.1,\n",
        "                 name='EEGTransformer'):\n",
        "        super(EEGTransformer, self).__init__(name=name)\n",
        "\n",
        "        # --- Positional Encoding Layer ---\n",
        "        self.pos_encoding_layer = PositionalEncoding()\n",
        "\n",
        "        # --- Transformer Encoder Block ---\n",
        "        self.multihead_attn = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=key_dim,\n",
        "            name='multihead_attention'\n",
        "        )\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_1\")\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(self.ff, activation=\"relu\"),\n",
        "                layers.Dense(num_channels), # Project back to the original embedding dimension\n",
        "            ],\n",
        "            name=\"feed_forward_network\"\n",
        "        )\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_2\")\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "        # --- Final Classifier ---\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.classifier = layers.Dense(output_dim, name=\"classifier\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the EEGTransformer model.\n",
        "        \"\"\"\n",
        "        # --- 1. Input Standardization ---\n",
        "        mean = tf.reduce_mean(inputs, axis=2, keepdims=True)\n",
        "        std = tf.math.reduce_std(inputs, axis=2, keepdims=True)\n",
        "        x = (inputs - mean) / (std + 1e-6)\n",
        "\n",
        "        # --- 2. Transpose and Add Positional Encoding ---\n",
        "        # Keras's MultiHeadAttention expects (batch, sequence, features).\n",
        "        # Our input is (batch, channels, timepoints). We transpose it first.\n",
        "        x = tf.transpose(x, perm=[0, 2, 1])  # to (batch, timepoints, channels)\n",
        "        \n",
        "        # Apply the positional encoding layer.\n",
        "        x = self.pos_encoding_layer(x)\n",
        "\n",
        "        # --- 3. Transformer Encoder Block ---\n",
        "        # --- Attention Sub-layer ---\n",
        "        attn_output = self.multihead_attn(query=x, value=x, key=x)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        # --- Feed-Forward Network Sub-layer ---\n",
        "        ffn_output = self.ffn(x)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        x = self.norm2(x + ffn_output)\n",
        "\n",
        "        # --- 4. Final Classifier ---\n",
        "        x = self.flatten(x)\n",
        "        output = self.classifier(x)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Supplementary Material\n",
        "# Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications\n",
        "#\n",
        "# ### EEGTransformer Class\n",
        "#\n",
        "# This notebook contains a TensorFlow/Keras re-implementation of the `EEGTransformer` class, originally presented in PyTorch. It is designed to be a faithful translation, allowing for integration into a TensorFlow-based pipeline.\n",
        "\n",
        "# Cell 2: Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# ### TensorFlow/Keras EEGTransformer Implementation\n",
        "#\n",
        "# This cell contains the complete and corrected class definitions. The positional encoding logic has been moved into its own dedicated Keras Layer to resolve the TensorFlow graph scope error.\n",
        "\n",
        "# Cell 4: PositionalEncoding and EEGTransformer Class Definitions\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Keras layer to create and apply positional encodings.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_timepoints, num_channels, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_timepoints = num_timepoints\n",
        "        self.num_channels = num_channels\n",
        "        # The positional encoding matrix is a non-trainable weight\n",
        "        self.positional_encoding = self.build_positional_encoding()\n",
        "\n",
        "    def build_positional_encoding(self):\n",
        "        positions = tf.range(start=0, limit=self.num_timepoints, delta=1, dtype=tf.float32)\n",
        "        channels = tf.range(start=0, limit=self.num_channels, delta=1, dtype=tf.float32)\n",
        "        angle_rates = 1 / (10000 ** ((2 * (channels // 2)) / tf.cast(self.num_channels, tf.float32)))\n",
        "        angle_rads = positions[:, tf.newaxis] * angle_rates[tf.newaxis, :]\n",
        "        sines = tf.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The input 'x' is expected to be of shape (batch, timepoints, channels)\n",
        "        return inputs + self.positional_encoding\n",
        "\n",
        "class EEGTransformer(keras.Model):\n",
        "    \"\"\"\n",
        "    TensorFlow/Keras model for EEG data using a Transformer architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_channels, num_timepoints, output_dim,\n",
        "                 num_heads, key_dim, ffn_intermediate_dim, dropout_rate=0.1,\n",
        "                 name='EEGTransformer'):\n",
        "        super(EEGTransformer, self).__init__(name=name)\n",
        "\n",
        "        # --- Positional Encoding Layer ---\n",
        "        self.pos_encoding_layer = PositionalEncoding(num_timepoints, num_channels)\n",
        "\n",
        "        # --- Transformer Encoder Block ---\n",
        "        self.multihead_attn = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=key_dim,\n",
        "            name='multihead_attention'\n",
        "        )\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_1\")\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ffn_intermediate_dim, activation=\"relu\"),\n",
        "                layers.Dense(num_channels), # Project back to the original embedding dimension\n",
        "            ],\n",
        "            name=\"feed_forward_network\"\n",
        "        )\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_2\")\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "        # --- Final Classifier ---\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.classifier = layers.Dense(output_dim, name=\"classifier\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the EEGTransformer model.\n",
        "        \"\"\"\n",
        "        # --- 1. Input Standardization ---\n",
        "        mean = tf.reduce_mean(inputs, axis=2, keepdims=True)\n",
        "        std = tf.math.reduce_std(inputs, axis=2, keepdims=True)\n",
        "        x = (inputs - mean) / (std + 1e-6)\n",
        "\n",
        "        # --- 2. Transpose and Add Positional Encoding ---\\\n",
        "        # Keras's MultiHeadAttention expects (batch, sequence, features).\n",
        "        # Our input is (batch, channels, timepoints). We transpose it first.\n",
        "        x = tf.transpose(x, perm=[0, 2, 1])  # to (batch, timepoints, channels)\n",
        "        # Apply the positional encoding layer.\n",
        "        x = self.pos_encoding_layer(x)\n",
        "\n",
        "        # --- 3. Transformer Encoder Block ---\n",
        "        # --- Attention Sub-layer ---\n",
        "        attn_output = self.multihead_attn(query=x, value=x, key=x)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        # --- Feed-Forward Network Sub-layer ---\n",
        "        ffn_output = self.ffn(x)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        x = self.norm2(x + ffn_output)\n",
        "\n",
        "        # --- 4. Final Classifier ---\n",
        "        x = self.flatten(x)\n",
        "        output = self.classifier(x)\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-07 01:06:43.141456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-08-07 01:06:46.791428: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2025-08-07 01:06:46.791763: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2025-08-07 01:06:46.791772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras    \n",
        "from keras import layers\n",
        "\n",
        "class EEGTransformer(keras.Model):\n",
        "    \"\"\"\n",
        "    Tensroflow Keras model for EEG data using a Transformer architecture.\n",
        "    This model is designed to process EEG data for tasks such as classification or regression.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_channels, num_timepoints, output_dim, \n",
        "                 num_heads, key_dim, ffn_intermediate_dim, dropout_rate=0.1,\n",
        "                 name='EEGTransformer'):\n",
        "        # Call the parent constructor\n",
        "        super(EEGTransformer, self).__init__(name=name)\n",
        "\n",
        "        # --- Store key parameters ---\n",
        "        # Number of channels in the EEG data (also the embedding dimension)\n",
        "        self.num_channels = num_channels\n",
        "        # Number of time points in the sequence\n",
        "        self.num_timepoints = num_timepoints\n",
        "        # The number of output classes for the final classifier\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # --- Positional Encoding ---\n",
        "        # Create the positional encoding matrix using TensorFlow operations\n",
        "        # This is a non-trainable part of the model\n",
        "        self.positional_encoding = None\n",
        "\n",
        "\n",
        "        # --- Transformer Encoder Block ---\n",
        "        # This block contains the core logic: Multi-Head Attention and Feed-Forward Network\n",
        "\n",
        "        # 1. Multi-Head Self-Attention Layer\n",
        "        # This layer learns the relationships between different time points\n",
        "\n",
        "        self.multihead_attn = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, \n",
        "            key_dim=key_dim,\n",
        "            name='multihead_attention'\n",
        "        ) \n",
        "\n",
        "        # 2. Layer Normalization for the attention block\n",
        "        # Stabilizes the output of the attention layer\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_1\")\n",
        "        \n",
        "        # 3. Dropout for the attention block\n",
        "        # A regularization technique to prevent overfitting\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "\n",
        "        # 4. Position-wise Feed-Forward Network (FFN)\n",
        "        # A simple two-layer MLP applied to each time point independently\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ffn_intermediate_dim, activation=\"relu\"),\n",
        "                layers.Dense(num_channels), # Project back to the original embedding dimension\n",
        "            ],\n",
        "            name=\"feed_forward_network\"\n",
        "        )\n",
        "\n",
        "        # 5. Layer Normalization for the FFN block\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_2\")\n",
        "\n",
        "        # 6. Dropout for the FFN block\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "        # --- Final Classifier ---\n",
        "        # This part takes the processed sequence and makes a final prediction\n",
        "\n",
        "        # Flattens the output of the transformer block into a single vector per trial\n",
        "        self.flatten = layers.Flatten()\n",
        "        # A dense layer to classify the flattened features into the output classes\n",
        "        self.classifier = layers.Dense(output_dim, name=\"classifier\")\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Builds the model's layers, including the positional encoding.\n",
        "        This method is called automatically by Keras when the model first sees data.\n",
        "        \"\"\"\n",
        "        if self.positional_encoding is None:\n",
        "            self.positional_encoding = self.build_positional_encoding()\n",
        "        super(EEGTransformer, self).build(input_shape)\n",
        "\n",
        "    # @classmethod\n",
        "    def build_positional_encoding(self):\n",
        "        \"\"\"\n",
        "        Creates the sinusoidal positional encoding matrix.\n",
        "        This is a direct TensorFlow implementation of the formula from the paper.\n",
        "        \"\"\"\n",
        "        # Create an array of positions (0, 1, 2, ..., num_timepoints-1)\n",
        "        positions = tf.range(start=0, limit=self.num_timepoints, delta=1, dtype=tf.float32)\n",
        "        # Create an array of dimensions/channels (0, 1, 2, ..., num_channels-1)\n",
        "        channels = tf.range(start=0, limit=self.num_channels, delta=1, dtype=tf.float32)\n",
        "\n",
        "        # Calculate the angle rates for the sine/cosine functions\n",
        "        # This is the 1 / (10000^(j/c)) part of the formula\n",
        "        angle_rates = 1 / (10000 ** ((2 * (channels // 2)) / tf.cast(self.num_channels, tf.float32)))\n",
        "\n",
        "        # Create the angle matrix by multiplying positions and rates\n",
        "        angle_rads = positions[:, tf.newaxis] * angle_rates[tf.newaxis, :]\n",
        "\n",
        "        # Apply sin to even indices in the array; 2i\n",
        "        sines = tf.sin(angle_rads[:, 0::2])\n",
        "        # Apply cos to odd indices in the array; 2i+1\n",
        "        cosines = tf.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        # Interleave the sines and cosines to form the final encoding matrix\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        # Add a batch dimension so it can be added to the input data\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        \n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    # @classmethod\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the EEGTransformer model.\n",
        "        \"\"\"\n",
        "        # --- 1. Input Standadization ---\n",
        "        # Standardize the input EEG data to have zero mean and unit variance\n",
        "        mean = tf.reduce_mean(inputs, axis=2, keepdims=True)\n",
        "        std = tf.math.reduce_std(inputs, axis=2, keepdims=True)\n",
        "        # Apply Z-score normalization\n",
        "        x = (inputs - mean) / (std + 1e-6)\n",
        "\n",
        "        # --- 2. Transpose and Add Positional Encoding ---\n",
        "        # Keras's MultiHeadAttention expects (batch, sequence, features)\n",
        "        # Our input is (batch, channels, timepoints). We transpose it first.\n",
        "        x = tf.transpose(x, perm=[0, 2, 1])  # to (batch, timepoints, channels)\n",
        "        # Add the positional encoding to the transposed inputs\n",
        "        x = x + self.positional_encoding\n",
        "\n",
        "        # --- 3. Transformer Encoder Block ---\n",
        "        # --- Attention Sub-layer ---\n",
        "        # Pass the data as query, key, and value to the multi-head attention layer\n",
        "        attn_output = self.multihead_attn(query=x, value=x, key=x)\n",
        "        # Add the attention output to the input (residual connection)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # --- Feed-Forward Network Sub-layer ---\n",
        "        # Pass the output through the feed-forward network\n",
        "        ffn_output = self.ffn(x)\n",
        "        # Apply dropout\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        # Apply the second skip-connection and layer normalization\n",
        "        x = self.norm2(x + ffn_output)\n",
        "\n",
        "\n",
        "        # --- 4. Final Classifier ---\n",
        "        # Flatten the output to prepare for classification\n",
        "        x = self.flatten(x)  # (batch, timepoints * channels)\n",
        "        # Pass through the classifier to get the final output\n",
        "        output = self.classifier(x)  # (batch, output_dim)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# --- Double-check of implemented functionalities ---\n",
        "# 1. Input Standardization: Implemented in the `call` method.\n",
        "# 2. Positional Encoding: Implemented in `build_positional_encoding` and added in `call`.\n",
        "# 3. Multi-Head Attention: Implemented using `layers.MultiHeadAttention`.\n",
        "# 4. Skip-Connections & Layer Norm: Implemented for both sub-layers (`x + attn_output`, `x + ffn_output`).\n",
        "# 5. Position-wise FFN: Implemented using `keras.Sequential` with two Dense layers.\n",
        "# 6. Final Classifier: Implemented using `layers.Flatten` and `layers.Dense`.\n",
        "# All core functionalities from the PyTorch notebook have been re-implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. SETUP PARAMETERS (Mirrors the PyTorch notebook example)\n",
        "This section defines the parameters for our synthetic data and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Synthetic Data Parameters ---\n",
        "# Set the number of channels in the synthetic EEG data.\n",
        "num_channels: int = 32\n",
        "# Set the number of time points (sequence length) in the synthetic EEG data.\n",
        "num_timepoints: int = 200\n",
        "# Set the number of trials (samples) in our synthetic batch.\n",
        "batch_size: int = 64\n",
        "# Set the number of output classes for our classification task.\n",
        "output_dim: int = 2  # L=2 for binary classification\n",
        "\n",
        "# --- Model Hyperparameters ---\n",
        "# These values are taken directly from the PyTorch notebook's example instantiation.\n",
        "# Number of attention heads in the Multi-Head Attention layer.\n",
        "num_heads: int = 8\n",
        "# Dimensionality of the key and query vectors in each attention head.\n",
        "key_dim: int = 512\n",
        "# The size of the hidden layer within the Feed-Forward Network (FFN).\n",
        "ffn_intermediate_dim: int = 2048\n",
        "\n",
        "# --- Training Parameters ---\n",
        "# The learning rate for the Adam optimizer.\n",
        "learning_rate: float = 0.001\n",
        "# The number of epochs to train for.\n",
        "epochs: int = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. GENERATE SYNTHETIC DATA\n",
        "This section creates random data to test the model, using TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Generating synthetic EEG data... ---\n",
            "Input data shape (X): (64, 32, 200)\n",
            "Labels shape (y): (64,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-07 01:06:49.357165: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2025-08-07 01:06:49.357593: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2025-08-07 01:06:49.360858: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"--- Generating synthetic EEG data... ---\")\n",
        "\n",
        "# Generate the input data 'X' with the shape (batch_size, num_channels, num_timepoints).\n",
        "# tf.random.normal is the TensorFlow equivalent of torch.randn.\n",
        "X: tf.Tensor = tf.random.normal((batch_size, num_channels, num_timepoints))\n",
        "\n",
        "# Generate the integer labels 'y' for binary classification (labels are 0 or 1).\n",
        "# np.random.randint is a straightforward way to create the labels.\n",
        "y: np.ndarray = np.random.randint(0, output_dim, (batch_size,))\n",
        "\n",
        "# Print the shapes to confirm they are correct.\n",
        "print(f\"Input data shape (X): {X.shape}\")\n",
        "print(f\"Labels shape (y): {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. INSTANTIATE AND COMPILE THE TENSORFLOW MODEL\n",
        "This section creates an instance of your TensorFlow EEGTransformer and\n",
        "prepares it for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Initializing the TensorFlow EEGTransformer model... ---\n",
            "Model: \"EEGTransformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " multihead_attention (MultiH  multiple                 536608    \n",
            " eadAttention)                                                   \n",
            "                                                                 \n",
            " layer_norm_1 (LayerNormaliz  multiple                 64        \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           multiple                  0         \n",
            "                                                                 \n",
            " feed_forward_network (Seque  (None, 200, 32)          133152    \n",
            " ntial)                                                          \n",
            "                                                                 \n",
            " layer_norm_2 (LayerNormaliz  multiple                 64        \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         multiple                  0         \n",
            "                                                                 \n",
            " flatten (Flatten)           multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  12802     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 682,690\n",
            "Trainable params: 682,690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Initializing the TensorFlow EEGTransformer model... ---\")\n",
        "\n",
        "# Instantiate your TensorFlow EEGTransformer class with the defined parameters.\n",
        "model: keras.Model = EEGTransformer(\n",
        "    num_channels=num_channels,\n",
        "    num_timepoints=num_timepoints,\n",
        "    output_dim=output_dim,\n",
        "    num_heads=num_heads,\n",
        "    key_dim=key_dim,\n",
        "    ffn_intermediate_dim=ffn_intermediate_dim\n",
        ")\n",
        "\n",
        "# Define the optimizer. tf.keras.optimizers.Adam is the equivalent of torch.optim.Adam.\n",
        "optimizer: keras.optimizers.Optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Define the loss function.\n",
        "# SparseCategoricalCrossentropy is used because our labels 'y' are integers (0, 1), not one-hot encoded.\n",
        "# from_logits=True is crucial because our model's final layer outputs raw scores (logits), not probabilities.\n",
        "loss_fn: keras.losses.Loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Compile the model. This configures the model with the optimizer, loss function,\n",
        "# and any metrics we want to track during training.\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Build the model by passing a sample of data through it. This is necessary to\n",
        "# print the summary.\n",
        "model.build(input_shape=(None, num_channels, num_timepoints))\n",
        "\n",
        "# Print a summary of the model's architecture and number of parameters.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. SETUP, DATA GENERATION, COMPILATION, AND TRAINING\n",
        "This single cell mirrors the functionality of the original PyTorch cell.\n",
        "It defines parameters, creates synthetic data, instantiates the model, compiles it, and runs the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n--- Starting model training... ---\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.1195 - accuracy: 0.5469\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.4959 - accuracy: 0.4531\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5997 - accuracy: 0.6562\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9979 - accuracy: 0.5469\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9084 - accuracy: 0.5469\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1638 - accuracy: 0.9531\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1268 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3811 - accuracy: 0.7656\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1390 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0240 - accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\\\n--- Starting model training... ---\")\n",
        "history = model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1  # Shows a progress bar\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n--- Training complete. ---\n",
            "Final Loss: 0.0240\n",
            "Final Accuracy: 1.0000\n",
            "\\n--- Model Summary ---\n",
            "Model: \"EEGTransformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " multihead_attention (MultiH  multiple                 536608    \n",
            " eadAttention)                                                   \n",
            "                                                                 \n",
            " layer_norm_1 (LayerNormaliz  multiple                 64        \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           multiple                  0         \n",
            "                                                                 \n",
            " feed_forward_network (Seque  (None, 200, 32)          133152    \n",
            " ntial)                                                          \n",
            "                                                                 \n",
            " layer_norm_2 (LayerNormaliz  multiple                 64        \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         multiple                  0         \n",
            "                                                                 \n",
            " flatten (Flatten)           multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  12802     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 682,690\n",
            "Trainable params: 682,690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# --- 5. Display Final Results ---\n",
        "print(\"\\\\n--- Training complete. ---\")\n",
        "final_loss = history.history['loss'][-1]\n",
        "final_accuracy = history.history['accuracy'][-1]\n",
        "print(f\"Final Loss: {final_loss:.4f}\")\n",
        "print(f\"Final Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "# You can also print the model summary after training, as .fit() will build it.\n",
        "print(\"\\\\n--- Model Summary ---\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer's Architecture for EEG Classification\n",
        "\n",
        "This section presents the \"standard\" approach for utilizing the Transformer encoder to classify EEG patterns for BCIs.\n",
        "\n",
        "#### Input Standardization and Positional Encoding\n",
        "\n",
        "Let the set of pairs $ D_{\\text{train}} = \\{(\\mathbf{X}_1,{y}_1),\\dots, (\\mathbf{X}_n,{y}_n)\\} $ denote $ n $ trials of EEG recordings where $ {y}_i $ is the scaler class variable with $ L $ possible labels (e.g., target and non-target in a binary classification) and $ \\mathbf{X}_i\\in \\mathbb{R}^{c\\times p} $ is the collection of EEG observations in the $ i^{\\text{th}} $ trial over $ c $ channels and $ p $ time points; that is to say,\n",
        "\n",
        "$$ \\mathbf{X}_i=[\\mathbf{x}_{i1},\\mathbf{x}_{i2},\\ldots,\\mathbf{x}_{ic}]^T, i=1,\\ldots,n\\,, $$\n",
        " \n",
        "with $ \\mathbf{x}_{ij}=[x_{ij1}, \\ldots, x_{ijp}]^T \\in \\mathbb{R}^{p\\times 1}, j=1,\\ldots,c $, where $ x_{ijk}, k=1, \\ldots, p $ denotes the $ k^{\\text{th}} $ element of vector $ \\mathbf{x}_{ij} $, and $ T $ denotes the transpose operator. The goal is to use $ D_{\\text{train}} $ and train a classifier $ \\psi: \\mathbb{R}^{c\\times p} \\rightarrow \\{0, 1, \\ldots, L-1\\} $ that maps a given $ \\mathbf{X} $ to a possible value of the class variable.\n",
        "\n",
        "It is common to apply standardization for each channel to make the sensory data across all channels comparable. \n",
        "In this regard, each $ \\mathbf{X}_i $ is converted to $ \\hat{\\mathbf{X}}_i $ where \n",
        "\n",
        "$$ \\hat{\\mathbf{X}}_i=[\\hat{\\mathbf{x}}_{i1},\\hat{\\mathbf{x}}_{i2},\\ldots,\\hat{\\mathbf{x}}_{ic}]^T, i=1,\\ldots,n\\,, $$\n",
        "\n",
        "and where $ \\hat{\\mathbf{x}}_{ij} = [\\hat{x}_{ij1}, \\ldots, \\hat{x}_{ijp}]^T $ such that \n",
        "\n",
        "$$ \\hat{x}_{ijk} = \\frac{{x}_{ijk}-m_{ij}}{s_{ij}}\\,, $$\n",
        "\n",
        "with $ m_{ij} $ and $ s_{ij} $ being the sample mean and sample standard deviation of vector $\\mathbf{x}_{ij} $ given by\n",
        "\n",
        "$$ m_{ij} = \\frac{1}{p} \\sum_{k=1}^p {x}_{ijk}\\,, $$\n",
        "$$ s_{ij} = \\sqrt{\\frac{1}{p} \\sum_{k=1}^p ({x}_{ijk}-m_{ij})^2}\\,, $$\n",
        "\n",
        "respectively.\n",
        "\n",
        "In order for the Transformer to make use of EEG recording orders, it is common to encode some information about the position of sequence elements in its input \\cite{vaswani_attention_2017}. This positional encoding is generally realized by adding each $ \\hat{\\mathbf{X}}_i $ to a matrix $ \\mathbf{P} \\in \\mathbb{R}^{c\\times p} $ that is defined based on trigonometric functions with different frequencies for each channel \\cite{vaswani_attention_2017}. As a result, we obtain\n",
        "\n",
        "$$ \\tilde{\\mathbf{X}}_i = \\hat{\\mathbf{X}}_i + \\mathbf{P}, \\,i=1,\\ldots, n, $$\n",
        "\n",
        "where the element on row (channel) $ j=1,\\ldots, c $, and column (time index) $ k=1, \\ldots, p $, of $ \\mathbf{P} $, denoted $ p_{jk} $ is given by\n",
        "\n",
        "$$ p_{jk} = \\begin{cases}\n",
        "\\text{\n",
        "\n",
        "sin}\\Big(k/10000^{j/c} \\Big), & \\text{for even } j \\\\\n",
        "\\text{cos}\\Big(k/10000^{j-1/c} \\Big), & \\text{for odd } j\n",
        "\\end{cases} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Self-Attention Mechanisms: Capturing Contexts for EEG Classification\n",
        "\n",
        "Capturing contexts is the essential concept that makes attention mechanism a promising operation for EEG classification. A context is simply another representation of an element of the input sequence (here one column of each $ \\tilde{\\mathbf{X}}_i $) based on its compatibility with other elements within the sequence. The most widely used attention operation for EEG classification is scaled dot-product self-attention, denoted $ \\text{SA}^d_{\\mathbf{V}, \\mathbf{K}, \\mathbf{Q}}(\\tilde{\\mathbf{X}}_i): \\mathbb{R}^{c\\times p} \\rightarrow \\mathbb{R}^{d\\times p} $, which was initially proposed and used for translation tasks \\cite{vaswani_attention_2017}. In particular,\n",
        "\n",
        "$$ \\text{SA}_{\\mathbf{V}, \\mathbf{K}, \\mathbf{Q}}^d(\\tilde{\\mathbf{X}}_i) = \\mathbf{V}\\tilde{\\mathbf{X}}_i\\times\\text{softmax}\\Big(\\frac{\\tilde{\\mathbf{X}}_i^T\\mathbf{K}^T\\mathbf{Q}\\tilde{\\mathbf{X}}_i}{\\sqrt{q}}\\Big)\\,, $$\n",
        "\n",
        "where $ \\mathbf{V} \\in \\mathbb{R}^{d\\times c} $, $ \\mathbf{K} \\in \\mathbb{R}^{q\\times c} $, $ \\mathbf{Q} \\in \\mathbb{R}^{q\\times c} $ are projection matrices that are learned in the training process, $ q $ is known as attention dimensionality, and  $ d $, which is generally a tuning parameter, denotes the dimensionality of the columns of the output matrix (context vectors). We use superscript $ d $ in $ \\text{SA}_{\\mathbf{V}, \\mathbf{K}, \\mathbf{Q}}^d(\\tilde{\\mathbf{X}}_i) $ to highlight the dimensionality of context vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Multi-Head Self-Attention\n",
        "\n",
        "Rather than a single self-attention operation, it is generally beneficial to apply multiple self-attentions in parallel. Using this operation, we view the compatibility of sequence elements using different learned projections. In this context, it is also common to refer to the output matrix of each self-attention as a head. In particular, the multi-head self-attention, denoted $ \\text{MSHA}(\\tilde{\\mathbf{X}}_i) : \\mathbb{R}^{c\\times p} \\rightarrow \\mathbb{R}^{d_h\\times p} $, is defined as\n",
        "\n",
        "$$ \\text{MSHA}^{d_h}(\\tilde{\\mathbf{X}}_i) = \\mathbf{W}[\\text{SA}^d_{\\mathbf{V}_1, \\mathbf{K}_1, \\mathbf{Q}_1}(\\tilde{\\mathbf{X}}_i)^T, \\ldots, \\text{SA}^d_{\\mathbf{V}_m, \\mathbf{K}_m, \\mathbf{Q}_m}(\\tilde{\\mathbf{X}}_i)^T]^T\\,, $$\n",
        "\n",
        "where $ \\mathbf{W}\\in \\mathbb{R}^{d_h\\times md} $ is another learnable projection matrix, $ m $ is the number of self-attentions used in (\\ref{MHSA}), which is also known as the number of heads, and $ d_h $ is the dimensionality of columns in the output of $ \\text{MSHA}^{d_h}(\\tilde{\\mathbf{X}}_i) $ operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Identity Skip-Connection and Layer Normalization\n",
        "\n",
        "To ensure the stability and efficacy of the training process, especially with the complex nature of EEG data, the Transformer encoder utilizes identity skip-connections \\cite{he_deep_2015} followed by layer normalization \\cite{Jimmy_2016}. Here we define these operations. Let $ \\text{SKP}\\big({\\text{LAY}(\\mathbf{Y})}\\big): \\mathbb{R}^{a\\times b}\\rightarrow \\mathbb{R}^{a\\times b}$ denote the identity skip-connection around a layer $\\text{LAY}(\\mathbf{Y}) $ (an operation) that operates on an input $ \\mathbf{Y} \\in \\mathbb{R}^{a\\times b} $ to produce an output of the same size as the input. Then\n",
        "\n",
        "$$ \\text{SKP}\\big({\\text{LAY}(\\mathbf{Y})}\\big) = \\mathbf{Y} + \\text{LAY}(\\mathbf{Y})\\,. $$\n",
        "\n",
        "That is to say, we simply add the output of $ \\text{LAY}(\\mathbf{Y}) $ to its input. \n",
        "Furthermore, let $\\text{LN}(\\mathbf{Y}):\\mathbb{R}^{a\\times b} \\rightarrow \\mathbb{R}^{a\\times b}$ denote the layer normalization applied to an $ (a > 1)\\times b $ matrix $ \\mathbf{Y} $ with elements $ y_{jk}, j=1,\\ldots,a, k=1,\\ldots,b $ where each row records measurements for a \"features\" (here, channel). Then, $ \\text{LN}(\\mathbf{Y}) $ produces $ \\mathring{\\mathbf{Y}} $, which is a matrix of the same size $ \\mathring{\\mathbf{Y}} $ with elements $ \\mathring{y}_{jk} $ where\n",
        "\n",
        "$$ \\mathring{y}_{jk} = \\frac{{y}_{jk}-m_{k}}{s_{k}}\\,, $$\n",
        "\n",
        "and where\n",
        "\n",
        "$$ m_{k} = \\frac{1}{a} \\sum_{j=1}^a {y}_{jk}\\,, $$\n",
        "$$ s_{k} = \\sqrt{\\frac{1}{a} \\sum_{j=1}^a ({y}_{jk}-m_{k})^2}\\,. $$\n",
        "\n",
        "In other words, $ \\mathring{\\mathbf{Y}} $ is a type of standardization where the sample mean and sample standard deviation are computed for each column of $ \\mathbf{Y} $ (in the EEG context means for each time point in the sequence) over all features. One place that these operations are used in the transformer encoder is to produce $ \\mathring{\\mathbf{X}}_i $ as follows:\n",
        "\n",
        "$$ \\mathring{\\mathbf{X}}_i = \\text{LN}\\Big(\\text{SKP}\\big({\\text{MSHA}^{c}(\\tilde{\\mathbf{X}}_i)}\\big)\\Big)\\,; $$\n",
        " \n",
        "\n",
        "that is, the skip-connection is used around the multi-head self-attention, which is then followed by layer normalization. Note that the use of skip-connection in (\\ref{outSKPLN}) enforces setting $ d_h $ defined in (\\ref{MHSA}) to $ c $, which is the number of channels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Position-wise Feed-Forward Networks\n",
        "\n",
        "The Transformer encoder utilizes a fully connected feed-forward network that transforms each element of a given sequence individually. Let $ \\mathbf{Y} \\in \\mathbb{R}^{a\\times b} $ be the generic matrix defined before. The effect of this position-wise feed-forward network operated on an input $ \\mathbf{Y} $, denoted \n",
        "$\\text{FFN}(\\mathbf{Y})$, is:\n",
        "\n",
        "$$ \\text{FFN}^s(\\mathbf{Y}) = [g(\\mathbf{y}_1), \\ldots, g(\\mathbf{y}_b)]\\,, $$\n",
        " \n",
        "\n",
        "where $ \\mathbf{y}_k, k=1,\\ldots, b $ are columns of $ \\mathbf{Y} $ and\n",
        "\n",
        "$$ g(\\mathbf{y}_k) = \\mathbf{W}_2\\times f(\\mathbf{W}_1\\mathbf{y}_k + \\mathbf{b}_1) + \\mathbf{b}_2\\,, $$\n",
        "\n",
        "where $ f(.) $ denotes an element-wise nonlinear activation function (e.g., ReLU), and $\\mathbf{W}_1\\in \\mathbb{R}^{r\\times a}$, and $\\mathbf{W}_2 \\in \\mathbb{R}^{s\\times r}$, and $\\mathbf{b}_1 \\in \\mathbb{R}^{r\\times 1}$, and $\\mathbf{b}_1 \\in \\mathbb{R}^{s\\times 1} $ are learnable matrices and vectors$ r $ is generally a tuning parameter. \n",
        "\n",
        "We use superscript $ s $ in $ \\text{FFN}^s(\\mathbf{Y}) $ to highlight the dimensionality of output vectors in (\\ref{FFN}). In the Transformer encoder, position-wise feed-forward network is used to produce an output $ {\\mathbf{O}}_i $ from $ \\mathring{\\mathbf{X}}_i $ obtained in (\\ref{outSKPLN}), which is then added to its input through the skip-connection, followed by layer normalization. This operation is characterized as follows:\n",
        "\n",
        "$$ {\\mathbf{O}}_i = \\text{LN}\\Big(\\text{SKP}\\big({\\text{FFN}^{c}(\\mathring{\\mathbf{X}}_i)}\\big)\\Big)\\,. $$\n",
        " \n",
        "\n",
        "Note that the use of skip-connection in (\\ref{outSKPLNN}) enforces setting $ s $ defined in (\\ref{FFN}) to $ c $. The classification can be performed by vectorizing $ {\\mathbf{O}}_i $ and using that as the input to a fully connected layer with a softmax activation function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'eq.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Read input text\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meq.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     13\u001b[0m     content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Apply the replacement\u001b[39;00m\n",
            "File \u001b[0;32m~/MTGNet/dipl_venv/lib64/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eq.txt'"
          ]
        }
      ],
      "source": [
        "# def replace_latex_delimiters(text):\n",
        "#     # Replace \\( and \\) with $\n",
        "#     text = text.replace(r\"\\(\", \"$\").replace(r\"\\)\", \"$\")\n",
        "    \n",
        "#     # Replace \\[ and \\] with $$\n",
        "#     text = text.replace(r\"\\[\", \"$$\").replace(r\"\\]\", \"$$\")\n",
        "    \n",
        "#     return text\n",
        "\n",
        "\n",
        "# # Read input text\n",
        "# with open('eq.txt', 'r') as file:\n",
        "#     content = file.read()\n",
        "\n",
        "# # Apply the replacement\n",
        "# modified_content = replace_latex_delimiters(content)\n",
        "\n",
        " \n",
        "# with open('output.txt', 'w') as file:\n",
        "#     file.write(modified_content)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dipl_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
