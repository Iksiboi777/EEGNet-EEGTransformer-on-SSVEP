{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTA3qNexKv3"
      },
      "source": [
        "## Supplementary Material\n",
        "Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications\n",
        "\n",
        "\n",
        "### EEGTransformer Class\n",
        "\n",
        "The `EEGTransformer` class is designed to leverage a transformer-based architecture tailored specifically for Electroencephalogram (EEG) data processing.\n",
        "\n",
        "#### Parameters:\n",
        "- `num_channels` (int): Specifies the number of channels in the EEG dataset.\n",
        "- `num_timepoints` (int): Indicates the number of time points or the sequence length in the EEG data.\n",
        "- `output_dim` (int): Defines the output dimensionality for the classifier layer.\n",
        "- `hidden_dim` (int): Specifies the hidden layer dimensionality.\n",
        "- `num_heads` (int): Determines the number of attention heads to be used in the multi-head self-attention mechanism.\n",
        "- `key_query_dim` (int): Denotes the dimensionality for the key/query pairs in the self-attention mechanism.\n",
        "- `hidden_ffn_dim` (int): Indicates the hidden layer dimensionality for the feed-forward network.\n",
        "- `intermediate_dim` (int): Refers to the dimensionality of the intermediate layer in the feed-forward network.\n",
        "- `ffn_output_dim` (int): Specifies the output size of the feed-forward network.\n",
        "\n",
        "#### Attributes:\n",
        "- `positional_encoding` (torch.Tensor): A tensor of shape `(num_channels, num_timepoints)` that imparts the sequence position information.\n",
        "- `multihead_attn` (nn.MultiheadAttention): Implements the multi-head self-attention mechanism.\n",
        "- `ffn` (nn.Sequential): Constructs a feed-forward network composed of a linear transformation followed by ReLU activation and another linear transformation.\n",
        "- `norm1` and `norm2` (nn.LayerNorm): Execute layer normalization.\n",
        "- `classifier` (nn.Linear): Deploys a final linear transformation layer to categorize the input into designated classes.\n",
        "\n",
        "#### Methods:\n",
        "- `forward(X)`: Outlines the forward propagation for the model.\n",
        "  - `X` (torch.Tensor): The input tensor for EEG data, which should have a shape of `(batch_size, num_channels, num_timepoints)`.\n",
        "\n",
        "  - **Steps**:\n",
        "    1. Standardize the input tensor.\n",
        "    2. Apply positional encoding.\n",
        "    3. Implement multi-head self-attention.\n",
        "    4. Reshape the attention output and apply layer normalization.\n",
        "    5. Forward the data through the feed-forward network.\n",
        "    6. Flatten the resultant tensor and direct it through a classifier layer.\n",
        "    7. Yield the final output.\n",
        "  \n",
        "### Notes:\n",
        "\n",
        "- The model applies layer normalization after the multi-head self-attention and feed-forward network stages.\n",
        "- Positional encoding is utilized to impart sequence position information to the model, which can either be relative or absolute.\n",
        "- The classifier layer flattens the model output and categorizes it into `output_dim` classes.\n",
        "\n",
        "### Usage:\n",
        "\n",
        "To employ the `EEGTransformer` model, instantiate the class using the desired parameters. Then, similar to any other PyTorch model, forward the input data to the model and utilize the returned output for either training or inference.\n",
        "\n",
        "```python\n",
        "# Sample Usage\n",
        "model = EEGTransformer(num_channels=32, num_timepoints=200, output_dim=2,\n",
        "                       hidden_dim=512, num_heads=8, key_query_dim=512,\n",
        "                       hidden_ffn_dim=512, intermediate_dim=2048,\n",
        "                       ffn_output_dim=32)\n",
        "                       \n",
        "input_data = torch.randn(64, 32, 200)\n",
        "output = model(input_data)\n",
        "```\n",
        "\n",
        "Ensure that the model is paired with a compatible loss function and optimizer for effective training. Depending on the specifics of the EEG dataset or application requirements, the model can be further refined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras    \n",
        "from keras import layers\n",
        "\n",
        "class EEGTransformer(keras.Model):\n",
        "    \"\"\"\n",
        "    Tensroflow Keras model for EEG data using a Transformer architecture.\n",
        "    This model is designed to process EEG data for tasks such as classification or regression.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_channels, num_timepoints, output_dim, \n",
        "                 num_heads, key_dim, ffn_intermediate_dim, dropout_rate=0.1,\n",
        "                 name='EEGTransformer'):\n",
        "        # Call the parent constructor\n",
        "        super(EEGTransformer, self).__init__(name=name)\n",
        "\n",
        "        # --- Store key parameters ---\n",
        "        # Number of channels in the EEG data (also the embedding dimension)\n",
        "        self.num_channels = num_channels\n",
        "        # Number of time points in the sequence\n",
        "        self.num_timepoints = num_timepoints\n",
        "        # The number of output classes for the final classifier\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # --- Positional Encoding ---\n",
        "        # Create the positional encoding matrix using TensorFlow operations\n",
        "        # This is a non-trainable part of the model\n",
        "        # self.positional_encoding = self.build_positional_encoding()\n",
        "\n",
        "\n",
        "        # --- Transformer Encoder Block ---\n",
        "        # This block contains the core logic: Multi-Head Attention and Feed-Forward Network\n",
        "\n",
        "        # 1. Multi-Head Self-Attention Layer\n",
        "        # This layer learns the relationships between different time points\n",
        "\n",
        "        self.multihead_attn = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, \n",
        "            key_dim=key_dim,\n",
        "            name='multihead_attention'\n",
        "        ) \n",
        "\n",
        "        # 2. Layer Normalization for the attention block\n",
        "        # Stabilizes the output of the attention layer\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_1\")\n",
        "        \n",
        "        # 3. Dropout for the attention block\n",
        "        # A regularization technique to prevent overfitting\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "\n",
        "        # 4. Position-wise Feed-Forward Network (FFN)\n",
        "        # A simple two-layer MLP applied to each time point independently\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ffn_intermediate_dim, activation=\"relu\"),\n",
        "                layers.Dense(num_channels), # Project back to the original embedding dimension\n",
        "            ],\n",
        "            name=\"feed_forward_network\"\n",
        "        )\n",
        "\n",
        "        # 5. Layer Normalization for the FFN block\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_2\")\n",
        "\n",
        "        # 6. Dropout for the FFN block\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "        # --- Final Classifier ---\n",
        "        # This part takes the processed sequence and makes a final prediction\n",
        "\n",
        "        # Flattens the output of the transformer block into a single vector per trial\n",
        "        self.flatten = layers.Flatten()\n",
        "        # A dense layer to classify the flattened features into the output classes\n",
        "        self.classifier = layers.Dense(output_dim, name=\"classifier\")\n",
        "\n",
        "@classmethod\n",
        "def build_positional_encoding(self):\n",
        "    \"\"\"\n",
        "    Creates the sinusoidal positional encoding matrix.\n",
        "    This is a direct TensorFlow implementation of the formula from the paper.\n",
        "    \"\"\"\n",
        "    # Create an array of positions (0, 1, 2, ..., num_timepoints-1)\n",
        "    positions = tf.range(start=0, limit=self.num_timepoints, delta=1, dtype=tf.float32)\n",
        "    # Create an array of dimensions/channels (0, 1, 2, ..., num_channels-1)\n",
        "    channels = tf.range(start=0, limit=self.num_channels, delta=1, dtype=tf.float32)\n",
        "\n",
        "    # Calculate the angle rates for the sine/cosine functions\n",
        "    # This is the 1 / (10000^(j/c)) part of the formula\n",
        "    angle_rates = 1 / (10000 ** ((2 * (channels // 2)) / tf.cast(self.num_channels, tf.float32)))\n",
        "\n",
        "    # Create the angle matrix by multiplying positions and rates\n",
        "    angle_rads = positions[:, tf.newaxis] * angle_rates[tf.newaxis, :]\n",
        "\n",
        "    # Apply sin to even indices in the array; 2i\n",
        "    sines = tf.sin(angle_rads[:, 0::2])\n",
        "    # Apply cos to odd indices in the array; 2i+1\n",
        "    cosines = tf.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    # Interleave the sines and cosines to form the final encoding matrix\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    # Add a batch dimension so it can be added to the input data\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    \n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "@classmethod\n",
        "def call(self, inputs):\n",
        "    \"\"\"\n",
        "    Defines the forward pass of the EEGTransformer model.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Standadization ---\n",
        "    # Standardize the input EEG data to have zero mean and unit variance\n",
        "    mean = tf.reduce_mean(inputs, axis=2, keepdims=True)\n",
        "    std = tf.math.reduce_std(inputs, axis=2, keepdims=True)\n",
        "    # Apply Z-score normalization\n",
        "    standardized_inputs = (inputs - mean) / (std + 1e-6)\n",
        "\n",
        "    # --- 2. Add Positional Encoding ---\n",
        "    # Add the positional encoding to the standardized inputs\n",
        "    x = x + self.positional_encoding[:, :tf.shape(x)[2], :]\n",
        "\n",
        "    # --- 3. Transformer Encoder Block ---\n",
        "    # Keras's MultiHeadAttention expects (batch, sequence, features)\n",
        "    # Our input is (batch, channels, timepoints). We need to transpose it. \n",
        "    x = tf.transpose(standardized_inputs, perm=[0, 2, 1])  # (batch, timepoints, channels)\n",
        "\n",
        "    # --- Attention Sub-layer ---\n",
        "    # Pass the data as query, key, and value to the multi-head attention layer\n",
        "    attn_output = self.multihead_attn(query=x, value=x, key=x)\n",
        "    # Add the attention output to the input (residual connection)\n",
        "    x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "    # --- Feed-Forward Network Sub-layer ---\n",
        "    # Pass the output through the feed-forward network\n",
        "    ffn_output = self.ffn(x)\n",
        "    # Apply dropout\n",
        "    ffn_output = self.dropout2(ffn_output)\n",
        "    # Apply the second skip-connection and layer normalization\n",
        "    x = self.norm2(x + ffn_output)\n",
        "\n",
        "\n",
        "    # --- 4. Final Classifier ---\n",
        "    # Flatten the output to prepare for classification\n",
        "    x = self.flatten(x)  # (batch, timepoints * channels)\n",
        "    # Pass through the classifier to get the final output\n",
        "    output = self.classifier(x)  # (batch, output_dim)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "# --- Double-check of implemented functionalities ---\n",
        "# 1. Input Standardization: Implemented in the `call` method.\n",
        "# 2. Positional Encoding: Implemented in `build_positional_encoding` and added in `call`.\n",
        "# 3. Multi-Head Attention: Implemented using `layers.MultiHeadAttention`.\n",
        "# 4. Skip-Connections & Layer Norm: Implemented for both sub-layers (`x + attn_output`, `x + ffn_output`).\n",
        "# 5. Position-wise FFN: Implemented using `keras.Sequential` with two Dense layers.\n",
        "# 6. Final Classifier: Implemented using `layers.Flatten` and `layers.Dense`.\n",
        "# All core functionalities from the PyTorch notebook have been re-implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. SETUP PARAMETERS (Mirrors the PyTorch notebook example)\n",
        "This section defines the parameters for our synthetic data and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Synthetic Data Parameters ---\n",
        "# Set the number of channels in the synthetic EEG data.\n",
        "num_channels: int = 32\n",
        "# Set the number of time points (sequence length) in the synthetic EEG data.\n",
        "num_timepoints: int = 200\n",
        "# Set the number of trials (samples) in our synthetic batch.\n",
        "batch_size: int = 64\n",
        "# Set the number of output classes for our classification task.\n",
        "output_dim: int = 2  # L=2 for binary classification\n",
        "\n",
        "# --- Model Hyperparameters ---\n",
        "# These values are taken directly from the PyTorch notebook's example instantiation.\n",
        "# Number of attention heads in the Multi-Head Attention layer.\n",
        "num_heads: int = 8\n",
        "# Dimensionality of the key and query vectors in each attention head.\n",
        "key_dim: int = 512\n",
        "# The size of the hidden layer within the Feed-Forward Network (FFN).\n",
        "ffn_intermediate_dim: int = 2048\n",
        "\n",
        "# --- Training Parameters ---\n",
        "# The learning rate for the Adam optimizer.\n",
        "learning_rate: float = 0.001\n",
        "# The number of epochs to train for.\n",
        "epochs: int = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. GENERATE SYNTHETIC DATA\n",
        "This section creates random data to test the model, using TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Generating synthetic EEG data... ---\n",
            "Input data shape (X): (64, 32, 200)\n",
            "Labels shape (y): (64,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"--- Generating synthetic EEG data... ---\")\n",
        "\n",
        "# Generate the input data 'X' with the shape (batch_size, num_channels, num_timepoints).\n",
        "# tf.random.normal is the TensorFlow equivalent of torch.randn.\n",
        "X: tf.Tensor = tf.random.normal((batch_size, num_channels, num_timepoints))\n",
        "\n",
        "# Generate the integer labels 'y' for binary classification (labels are 0 or 1).\n",
        "# np.random.randint is a straightforward way to create the labels.\n",
        "y: np.ndarray = np.random.randint(0, output_dim, (batch_size,))\n",
        "\n",
        "# Print the shapes to confirm they are correct.\n",
        "print(f\"Input data shape (X): {X.shape}\")\n",
        "print(f\"Labels shape (y): {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. INSTANTIATE AND COMPILE THE TENSORFLOW MODEL\n",
        "This section creates an instance of your TensorFlow EEGTransformer and\n",
        "prepares it for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Initializing the TensorFlow EEGTransformer model... ---\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "Unimplemented `tf.keras.Model.call()`: if you intend to create a `Model` with the Functional API, please provide `inputs` and `outputs` arguments. Otherwise, subclass `Model` with an overridden `call()` method.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss_fn, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Build the model by passing a sample of data through it. This is necessary to\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print the summary.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_timepoints\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Print a summary of the model's architecture and number of parameters.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
            "File \u001b[0;32m~/MTGNet/dipl_venv/lib64/python3.9/site-packages/keras/engine/training.py:513\u001b[0m, in \u001b[0;36mModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can only call `build()` on a model if its \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`call()` method accepts an `inputs` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m     )\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mInvalidArgumentError, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot build your model by calling `build` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif your layers do not support float type inputs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`call` is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m     )\n",
            "File \u001b[0;32m~/MTGNet/dipl_venv/lib64/python3.9/site-packages/keras/engine/training.py:588\u001b[0m, in \u001b[0;36mModel.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_in_current_and_subclasses\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    565\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the model on new inputs and returns the outputs as tensors.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m    In this case `call()` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;124;03m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnimplemented `tf.keras.Model.call()`: if you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintend to create a `Model` with the Functional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI, please provide `inputs` and `outputs` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    592\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments. Otherwise, subclass `Model` with an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverridden `call()` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    594\u001b[0m     )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unimplemented `tf.keras.Model.call()`: if you intend to create a `Model` with the Functional API, please provide `inputs` and `outputs` arguments. Otherwise, subclass `Model` with an overridden `call()` method."
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Initializing the TensorFlow EEGTransformer model... ---\")\n",
        "\n",
        "# Instantiate your TensorFlow EEGTransformer class with the defined parameters.\n",
        "model: keras.Model = EEGTransformer(\n",
        "    num_channels=num_channels,\n",
        "    num_timepoints=num_timepoints,\n",
        "    output_dim=output_dim,\n",
        "    num_heads=num_heads,\n",
        "    key_dim=key_dim,\n",
        "    ffn_intermediate_dim=ffn_intermediate_dim\n",
        ")\n",
        "\n",
        "# Define the optimizer. tf.keras.optimizers.Adam is the equivalent of torch.optim.Adam.\n",
        "optimizer: keras.optimizers.Optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Define the loss function.\n",
        "# SparseCategoricalCrossentropy is used because our labels 'y' are integers (0, 1), not one-hot encoded.\n",
        "# from_logits=True is crucial because our model's final layer outputs raw scores (logits), not probabilities.\n",
        "loss_fn: keras.losses.Loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Compile the model. This configures the model with the optimizer, loss function,\n",
        "# and any metrics we want to track during training.\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Build the model by passing a sample of data through it. This is necessary to\n",
        "# print the summary.\n",
        "model.build(input_shape=(None, num_channels, num_timepoints))\n",
        "\n",
        "# Print a summary of the model's architecture and number of parameters.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# num_channels=32\n",
        "# num_timepoints=200\n",
        "# batch_size = 64\n",
        "\n",
        "# X = torch.randn(batch_size, num_channels, num_timepoints)\n",
        "# y = torch.randint(0, 2, (batch_size,))  # L=2 for binary classification\n",
        "\n",
        "# # Model, Loss and Optimizer\n",
        "# model = EEGTransformer(num_channels, num_timepoints, output_dim=2,\n",
        "#                        hidden_dim=512, num_heads=8, key_query_dim=512,\n",
        "#                        hidden_ffn_dim=512, intermediate_dim=2048,\n",
        "#                        ffn_output_dim=num_channels)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training loop\n",
        "# epochs = 10\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     optimizer.zero_grad()\n",
        "#     outputs = model(X)\n",
        "#     loss = criterion(outputs, y)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# # once the model is trained, it can be tested on unseen EEG test examples\n",
        "# # also, different model selection techniques (e.g. cross-validation methods) can be implemented within the training loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer's Architecture for EEG Classification\n",
        "\n",
        "This section presents the \"standard\" approach for utilizing the Transformer encoder to classify EEG patterns for BCIs.\n",
        "\n",
        "#### Input Standardization and Positional Encoding\n",
        "\n",
        "Let the set of pairs $ D_{\\text{train}} = \\{(\\mathbf{X}_1,{y}_1),\\dots, (\\mathbf{X}_n,{y}_n)\\} $ denote $ n $ trials of EEG recordings where $ {y}_i $ is the scaler class variable with $ L $ possible labels (e.g., target and non-target in a binary classification) and $ \\mathbf{X}_i\\in \\mathbb{R}^{c\\times p} $ is the collection of EEG observations in the $ i^{\\text{th}} $ trial over $ c $ channels and $ p $ time points; that is to say,\n",
        "\n",
        "$$ \\mathbf{X}_i=[\\mathbf{x}_{i1},\\mathbf{x}_{i2},\\ldots,\\mathbf{x}_{ic}]^T, i=1,\\ldots,n\\,, $$\n",
        " \n",
        "with $ \\mathbf{x}_{ij}=[x_{ij1}, \\ldots, x_{ijp}]^T \\in \\mathbb{R}^{p\\times 1}, j=1,\\ldots,c $, where $ x_{ijk}, k=1, \\ldots, p $ denotes the $ k^{\\text{th}} $ element of vector $ \\mathbf{x}_{ij} $, and $ T $ denotes the transpose operator. The goal is to use $ D_{\\text{train}} $ and train a classifier $ \\psi: \\mathbb{R}^{c\\times p} \\rightarrow \\{0, 1, \\ldots, L-1\\} $ that maps a given $ \\mathbf{X} $ to a possible value of the class variable.\n",
        "\n",
        "It is common to apply standardization for each channel to make the sensory data across all channels comparable. \n",
        "In this regard, each $ \\mathbf{X}_i $ is converted to $ \\hat{\\mathbf{X}}_i $ where \n",
        "\n",
        "$$ \\hat{\\mathbf{X}}_i=[\\hat{\\mathbf{x}}_{i1},\\hat{\\mathbf{x}}_{i2},\\ldots,\\hat{\\mathbf{x}}_{ic}]^T, i=1,\\ldots,n\\,, $$\n",
        "\n",
        "and where $ \\hat{\\mathbf{x}}_{ij} = [\\hat{x}_{ij1}, \\ldots, \\hat{x}_{ijp}]^T $ such that \n",
        "\n",
        "$$ \\hat{x}_{ijk} = \\frac{{x}_{ijk}-m_{ij}}{s_{ij}}\\,, $$\n",
        "\n",
        "with $ m_{ij} $ and $ s_{ij} $ being the sample mean and sample standard deviation of vector $\\mathbf{x}_{ij} $ given by\n",
        "\n",
        "$$ m_{ij} = \\frac{1}{p} \\sum_{k=1}^p {x}_{ijk}\\,, $$\n",
        "$$ s_{ij} = \\sqrt{\\frac{1}{p} \\sum_{k=1}^p ({x}_{ijk}-m_{ij})^2}\\,, $$\n",
        "\n",
        "respectively.\n",
        "\n",
        "In order for the Transformer to make use of EEG recording orders, it is common to encode some information about the position of sequence elements in its input \\cite{vaswani_attention_2017}. This positional encoding is generally realized by adding each $ \\hat{\\mathbf{X}}_i $ to a matrix $ \\mathbf{P} \\in \\mathbb{R}^{c\\times p} $ that is defined based on trigonometric functions with different frequencies for each channel \\cite{vaswani_attention_2017}. As a result, we obtain\n",
        "\n",
        "$$ \\tilde{\\mathbf{X}}_i = \\hat{\\mathbf{X}}_i + \\mathbf{P}, \\,i=1,\\ldots, n, $$\n",
        "\n",
        "where the element on row (channel) $ j=1,\\ldots, c $, and column (time index) $ k=1, \\ldots, p $, of $ \\mathbf{P} $, denoted $ p_{jk} $ is given by\n",
        "\n",
        "$$ p_{jk} = \\begin{cases}\n",
        "\\text{\n",
        "\n",
        "sin}\\Big(k/10000^{j/c} \\Big), & \\text{for even } j \\\\\n",
        "\\text{cos}\\Big(k/10000^{j-1/c} \\Big), & \\text{for odd } j\n",
        "\\end{cases} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Self-Attention Mechanisms: Capturing Contexts for EEG Classification\n",
        "\n",
        "Capturing contexts is the essential concept that makes attention mechanism a promising operation for EEG classification. A context is simply another representation of an element of the input sequence (here one column of each $ \\tilde{\\mathbf{X}}_i $) based on its compatibility with other elements within the sequence. The most widely used attention operation for EEG classification is scaled dot-product self-attention, denoted $ \\text{SA}^d_{\\mathbf{V}, \\mathbf{K}, \\mathbf{Q}}(\\tilde{\\mathbf{X}}_i): \\mathbb{R}^{c\\times p} \\rightarrow \\mathbb{R}^{d\\times p} $, which was initially proposed and used for translation tasks \\cite{vaswani_attention_2017}. In particular,\n",
        "\n",
        "$$ \\text{SA}_{\\mathbf{V}, \\mathbf{K}, \\mathbf{Q}}^d(\\tilde{\\mathbf{X}}_i) = \\mathbf{V}\\tilde{\\mathbf{X}}_i\\times\\text{softmax}\\Big(\\frac{\\tilde{\\mathbf{X}}_i^T\\mathbf{K}^T\\mathbf{Q}\\tilde{\\mathbf{X}}_i}{\\sqrt{q}}\\Big)\\,, $$\n",
        "\n",
        "where $ \\mathbf{V} \\in \\mathbb{R}^{d\\times c} $, $ \\mathbf{K} \\in \\mathbb{R}^{q\\times c} $, $ \\mathbf{Q} \\in \\mathbb{R}^{q\\times c} $ are projection matrices that are learned in the training process, $ q $ is known as attention dimensionality, and  $ d $, which is generally a tuning parameter, denotes the dimensionality of the columns of the output matrix (context vectors). We use superscript $ d $ in $ \\text{SA}_{\\mathbf{V}, \\mathbf{K}, \\mathbf{Q}}^d(\\tilde{\\mathbf{X}}_i) $ to highlight the dimensionality of context vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Multi-Head Self-Attention\n",
        "\n",
        "Rather than a single self-attention operation, it is generally beneficial to apply multiple self-attentions in parallel. Using this operation, we view the compatibility of sequence elements using different learned projections. In this context, it is also common to refer to the output matrix of each self-attention as a head. In particular, the multi-head self-attention, denoted $ \\text{MSHA}(\\tilde{\\mathbf{X}}_i) : \\mathbb{R}^{c\\times p} \\rightarrow \\mathbb{R}^{d_h\\times p} $, is defined as\n",
        "\n",
        "$$ \\text{MSHA}^{d_h}(\\tilde{\\mathbf{X}}_i) = \\mathbf{W}[\\text{SA}^d_{\\mathbf{V}_1, \\mathbf{K}_1, \\mathbf{Q}_1}(\\tilde{\\mathbf{X}}_i)^T, \\ldots, \\text{SA}^d_{\\mathbf{V}_m, \\mathbf{K}_m, \\mathbf{Q}_m}(\\tilde{\\mathbf{X}}_i)^T]^T\\,, $$\n",
        "\n",
        "where $ \\mathbf{W}\\in \\mathbb{R}^{d_h\\times md} $ is another learnable projection matrix, $ m $ is the number of self-attentions used in (\\ref{MHSA}), which is also known as the number of heads, and $ d_h $ is the dimensionality of columns in the output of $ \\text{MSHA}^{d_h}(\\tilde{\\mathbf{X}}_i) $ operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Identity Skip-Connection and Layer Normalization\n",
        "\n",
        "To ensure the stability and efficacy of the training process, especially with the complex nature of EEG data, the Transformer encoder utilizes identity skip-connections \\cite{he_deep_2015} followed by layer normalization \\cite{Jimmy_2016}. Here we define these operations. Let $ \\text{SKP}\\big({\\text{LAY}(\\mathbf{Y})}\\big): \\mathbb{R}^{a\\times b}\\rightarrow \\mathbb{R}^{a\\times b}$ denote the identity skip-connection around a layer $\\text{LAY}(\\mathbf{Y}) $ (an operation) that operates on an input $ \\mathbf{Y} \\in \\mathbb{R}^{a\\times b} $ to produce an output of the same size as the input. Then\n",
        "\n",
        "$$ \\text{SKP}\\big({\\text{LAY}(\\mathbf{Y})}\\big) = \\mathbf{Y} + \\text{LAY}(\\mathbf{Y})\\,. $$\n",
        "\n",
        "That is to say, we simply add the output of $ \\text{LAY}(\\mathbf{Y}) $ to its input. \n",
        "Furthermore, let $\\text{LN}(\\mathbf{Y}):\\mathbb{R}^{a\\times b} \\rightarrow \\mathbb{R}^{a\\times b}$ denote the layer normalization applied to an $ (a > 1)\\times b $ matrix $ \\mathbf{Y} $ with elements $ y_{jk}, j=1,\\ldots,a, k=1,\\ldots,b $ where each row records measurements for a \"features\" (here, channel). Then, $ \\text{LN}(\\mathbf{Y}) $ produces $ \\mathring{\\mathbf{Y}} $, which is a matrix of the same size $ \\mathring{\\mathbf{Y}} $ with elements $ \\mathring{y}_{jk} $ where\n",
        "\n",
        "$$ \\mathring{y}_{jk} = \\frac{{y}_{jk}-m_{k}}{s_{k}}\\,, $$\n",
        "\n",
        "and where\n",
        "\n",
        "$$ m_{k} = \\frac{1}{a} \\sum_{j=1}^a {y}_{jk}\\,, $$\n",
        "$$ s_{k} = \\sqrt{\\frac{1}{a} \\sum_{j=1}^a ({y}_{jk}-m_{k})^2}\\,. $$\n",
        "\n",
        "In other words, $ \\mathring{\\mathbf{Y}} $ is a type of standardization where the sample mean and sample standard deviation are computed for each column of $ \\mathbf{Y} $ (in the EEG context means for each time point in the sequence) over all features. One place that these operations are used in the transformer encoder is to produce $ \\mathring{\\mathbf{X}}_i $ as follows:\n",
        "\n",
        "$$ \\mathring{\\mathbf{X}}_i = \\text{LN}\\Big(\\text{SKP}\\big({\\text{MSHA}^{c}(\\tilde{\\mathbf{X}}_i)}\\big)\\Big)\\,; $$\n",
        " \n",
        "\n",
        "that is, the skip-connection is used around the multi-head self-attention, which is then followed by layer normalization. Note that the use of skip-connection in (\\ref{outSKPLN}) enforces setting $ d_h $ defined in (\\ref{MHSA}) to $ c $, which is the number of channels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Position-wise Feed-Forward Networks\n",
        "\n",
        "The Transformer encoder utilizes a fully connected feed-forward network that transforms each element of a given sequence individually. Let $ \\mathbf{Y} \\in \\mathbb{R}^{a\\times b} $ be the generic matrix defined before. The effect of this position-wise feed-forward network operated on an input $ \\mathbf{Y} $, denoted \n",
        "$\\text{FFN}(\\mathbf{Y})$, is:\n",
        "\n",
        "$$ \\text{FFN}^s(\\mathbf{Y}) = [g(\\mathbf{y}_1), \\ldots, g(\\mathbf{y}_b)]\\,, $$\n",
        " \n",
        "\n",
        "where $ \\mathbf{y}_k, k=1,\\ldots, b $ are columns of $ \\mathbf{Y} $ and\n",
        "\n",
        "$$ g(\\mathbf{y}_k) = \\mathbf{W}_2\\times f(\\mathbf{W}_1\\mathbf{y}_k + \\mathbf{b}_1) + \\mathbf{b}_2\\,, $$\n",
        "\n",
        "where $ f(.) $ denotes an element-wise nonlinear activation function (e.g., ReLU), and $\\mathbf{W}_1\\in \\mathbb{R}^{r\\times a}$, and $\\mathbf{W}_2 \\in \\mathbb{R}^{s\\times r}$, and $\\mathbf{b}_1 \\in \\mathbb{R}^{r\\times 1}$, and $\\mathbf{b}_1 \\in \\mathbb{R}^{s\\times 1} $ are learnable matrices and vectorsâ€”$ r $ is generally a tuning parameter. \n",
        "\n",
        "We use superscript $ s $ in $ \\text{FFN}^s(\\mathbf{Y}) $ to highlight the dimensionality of output vectors in (\\ref{FFN}). In the Transformer encoder, position-wise feed-forward network is used to produce an output $ {\\mathbf{O}}_i $ from $ \\mathring{\\mathbf{X}}_i $ obtained in (\\ref{outSKPLN}), which is then added to its input through the skip-connection, followed by layer normalization. This operation is characterized as follows:\n",
        "\n",
        "$$ {\\mathbf{O}}_i = \\text{LN}\\Big(\\text{SKP}\\big({\\text{FFN}^{c}(\\mathring{\\mathbf{X}}_i)}\\big)\\Big)\\,. $$\n",
        " \n",
        "\n",
        "Note that the use of skip-connection in (\\ref{outSKPLNN}) enforces setting $ s $ defined in (\\ref{FFN}) to $ c $. The classification can be performed by vectorizing $ {\\mathbf{O}}_i $ and using that as the input to a fully connected layer with a softmax activation function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_latex_delimiters(text):\n",
        "    # Replace \\( and \\) with $\n",
        "    text = text.replace(r\"\\(\", \"$\").replace(r\"\\)\", \"$\")\n",
        "    \n",
        "    # Replace \\[ and \\] with $$\n",
        "    text = text.replace(r\"\\[\", \"$$\").replace(r\"\\]\", \"$$\")\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "# Read input text\n",
        "with open('eq.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Apply the replacement\n",
        "modified_content = replace_latex_delimiters(content)\n",
        "\n",
        " \n",
        "with open('output.txt', 'w') as file:\n",
        "    file.write(modified_content)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dipl_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
